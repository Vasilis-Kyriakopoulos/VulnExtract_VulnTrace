{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e11f9781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-08 12:20:39.434883: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60f681c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/VulnExtractData/ffmpeg_test/Train',\n",
       " './data/VulnExtractData/ffmpeg_test/Test')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data_path = './data/VulnExtractData/ffmpeg_test'\n",
    "train_path = data_path + '/Train'\n",
    "test_path = data_path + '/Test'\n",
    "train_path,test_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "785322e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = ['AF','BF','CL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f7fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def train_model_svm(X_train,Y_train,X_test,Y_test):\n",
    "    param_grid = {\n",
    "        'svc__C': [0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'svc__degree': [2, 3, 4],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "\n",
    "    pipeline = make_pipeline(TfidfVectorizer(ngram_range=(1,1)), SVC(probability=True))\n",
    "    grid_search = GridSearchCV(pipeline,param_grid,verbose = 2,n_jobs = -1)\n",
    "\n",
    "    grid_search.fit(X_train,Y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    Y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, digits=4))\n",
    "\n",
    "def train_model_svm_with_features(X_train,Y_train,X_test,Y_test):\n",
    "    param_grid = {\n",
    "        'svc__C': [0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'svc__degree': [2, 3, 4],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('tfidf', TfidfVectorizer(), 'Description'),  # only your text column name here\n",
    "        ('scaler', StandardScaler(), X_train.columns[1:])\n",
    "    ], remainder='drop')\n",
    "\n",
    "    pipeline = make_pipeline(preprocessor, SVC(probability=True))\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline,param_grid,verbose = 2,n_jobs = -1)\n",
    "\n",
    "    grid_search.fit(X_train,Y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    Y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, digits=4))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe482e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_svm_with_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#train_model_svm(X_train,Y_train,X_test,Y_test)\u001b[39;00m\n\u001b[32m     55\u001b[39m X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrain_model_svm_with_features\u001b[49m(X_train,Y_train,X_test,Y_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_model_svm_with_features' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def prepare_data(task_id,with_features = False):    \n",
    "    folder_train_path = train_path + '/' + task_id\n",
    "\n",
    "    all_train_files = []\n",
    "    for root, dirs, files in os.walk(folder_train_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_train_files.append(full_path)\n",
    "\n",
    "    dfs = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    folder_test_path = test_path + '/' + task_id\n",
    "\n",
    "    all_test_files = []\n",
    "    for root, dirs, files in os.walk(folder_test_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_test_files.append(full_path)\n",
    "\n",
    "    dfs_train = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    dfs_test = [pd.read_csv(file) for file in all_test_files]\n",
    "\n",
    "    df_train = pd.concat(dfs_train, ignore_index=True)\n",
    "\n",
    "    df_test  = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "    #use only Description for training\n",
    "    X_train_descriptions = df_train['Description']\n",
    "    X_test_descriptions = df_test['Description']\n",
    "\n",
    "    if with_features == True:\n",
    "        X_train_features = df_train.drop(columns=['Description', 'label', 'CVE_ID'])\n",
    "        X_test_features = df_test.drop(columns=['Description', 'label', 'CVE_ID'])\n",
    "\n",
    "        X_train = pd.concat([X_train_descriptions,X_train_features],axis=1)\n",
    "        X_test = pd.concat([X_test_descriptions,X_test_features],axis=1)\n",
    "    else:\n",
    "        X_train = X_train_descriptions\n",
    "        X_test = X_test_descriptions\n",
    "\n",
    "    Y_train = df_train['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "    Y_test = df_test['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "   \n",
    "    return X_train,Y_train,X_test,Y_test\n",
    "\n",
    "\n",
    "\n",
    "for task_id in task_list:\n",
    "    X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=False)\n",
    "    #train_model_svm(X_train,Y_train,X_test,Y_test)\n",
    "    X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=True)\n",
    "    train_model_svm_with_features(X_train,Y_train,X_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d27f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.texts = df['Description']\n",
    "        self.labels =  df['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "class TextFeatureDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        feature_cols = [col for col in df.columns if col not in ['Description', 'CVE_ID', 'label']]\n",
    "        self.max_len = max_len\n",
    "        self.features = df[feature_cols].astype(float)\n",
    "        self.texts = df['Description']\n",
    "        self.labels =  df['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        features = self.features.iloc[index].to_numpy(dtype=np.float32, copy=True)\n",
    "        features = torch.from_numpy(features).contiguous() \n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'features': features,\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }    \n",
    "\n",
    "class BertClassifierWithFeatures(nn.Module):\n",
    "    def __init__(self,bert_model_name='bert-base-uncased',num_additional_features=16, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name,num_labels=num_classes)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + num_additional_features, num_classes)\n",
    "        \n",
    "    def forward(self,input_ids,attention_mask,features):\n",
    "        outputs = self.bert(input_ids,attention_mask = attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat((cls_embedding, features), dim=1)\n",
    "        outputs = self.classifier(combined)\n",
    "        return outputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    features = [item['features'] for item in batch]\n",
    "    \n",
    "  \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'features': features,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def train_model(model,model_class ,train_loader,val_loader, device, optimizer, scheduler, epochs=3):\n",
    "    model.to(device)\n",
    "    losses=[],accuracies= []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        total_loss = 0\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            features = batch['features'].to(device)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            if model_class  ==  BertClassifierWithFeatures:\n",
    "                outputs = model(input_ids = input_ids,attention_mask = attention_mask,features=features)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                logits = torch.softmax(outputs,dim=1)\n",
    "                preds = torch.argmax(logits, dim=1)\n",
    "            else:\n",
    "                outputs = model(input_ids,attention_mask = attention_mask,labels=labels)\n",
    "                loss = outputs.loss\n",
    "                preds = torch.argmax(outputs.logits)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "            \n",
    "            if i % 20 == 0 :\n",
    "                print(f'Epoch:{epoch} Step:{i+1}/{len(train_loader)} Loss:{loss.item()}')\n",
    "\n",
    "        avg_loss = total_loss/len(train_loader)\n",
    "        accuracy = n_correct/n_samples\n",
    "\n",
    "        losses.append(avg_loss)\n",
    "        accuracies.append(accuracy)\n",
    "        losses.append(total_loss/len(train_loader))\n",
    "            \n",
    "\n",
    "        \n",
    "def run_training(model_name,model_class,tokenizer,df_train,df_val,epochs = 3,learning_rate = 5e-5):\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()      # releases cached GPU memory back to the driver\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    train_dataset = TextFeatureDataset(df_train,tokenizer)\n",
    "    val_dataset = TextFeatureDataset(df_val,tokenizer)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    if model_class == BertClassifierWithFeatures:\n",
    "        num_features = train_dataset.features.shape[1]  # <- infer!\n",
    "    #   print(num_features)\n",
    "        model = model_class(model_name,num_additional_features = num_features, num_classes=2)\n",
    "        \n",
    "    else: \n",
    "        model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    loss_list,accuracy_list = train_model(model,\n",
    "                                          model_class=model_class,\n",
    "                                          train_loader=train_loader, \n",
    "                                          device=device,\n",
    "                                          optimizer=optimizer, \n",
    "                                          scheduler=scheduler, \n",
    "                                          epochs=epochs\n",
    "                                        )\n",
    "    \n",
    "\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished cleanly.\")\n",
    "    return model\n",
    "    \n",
    "\n",
    "def evaluate_model(model,model_class,df_train,tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    test_dataset = TextFeatureDataset(df_train,tokenizer)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            features = batch['features'].to(device)\n",
    "            \n",
    "       \n",
    "            if model_class  ==  BertClassifierWithFeatures:\n",
    "                outputs = model(input_ids = input_ids,attention_mask = attention_mask,features=features)\n",
    "                logits = torch.softmax(outputs,dim=1)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "                preds = torch.argmax(logits,dim = 1)\n",
    "                \n",
    "            else:\n",
    "                outputs = model(input_ids,attention_mask = attention_mask)\n",
    "                preds = torch.argmax(outputs.logits,dim = 1)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "     \n",
    "    print(100 * n_correct/ n_samples)\n",
    "    print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive'], digits=4))\n",
    "    return 100 * n_correct/ n_samples\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d540084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/238 Loss:0.4110304117202759\n",
      "Epoch:0 Step:21/238 Loss:0.269157350063324\n",
      "Epoch:0 Step:41/238 Loss:0.25802895426750183\n",
      "Epoch:0 Step:61/238 Loss:0.4497813880443573\n",
      "Epoch:0 Step:81/238 Loss:0.1657324880361557\n",
      "Epoch:0 Step:101/238 Loss:0.5277336239814758\n",
      "Epoch:0 Step:121/238 Loss:0.6443719267845154\n",
      "Epoch:0 Step:141/238 Loss:0.22751185297966003\n",
      "Epoch:0 Step:161/238 Loss:0.09248163551092148\n",
      "Epoch:0 Step:181/238 Loss:0.16462604701519012\n",
      "Epoch:0 Step:201/238 Loss:0.2235037088394165\n",
      "Epoch:0 Step:221/238 Loss:0.3200359642505646\n",
      "Epoch:1 Step:1/238 Loss:0.04552358761429787\n",
      "Epoch:1 Step:21/238 Loss:0.4247938096523285\n",
      "Epoch:1 Step:41/238 Loss:0.9360978603363037\n",
      "Epoch:1 Step:61/238 Loss:0.13442298769950867\n",
      "Epoch:1 Step:81/238 Loss:0.012406101450324059\n",
      "Epoch:1 Step:101/238 Loss:0.00820823572576046\n",
      "Epoch:1 Step:121/238 Loss:0.03346443921327591\n",
      "Epoch:1 Step:141/238 Loss:0.05825212597846985\n",
      "Epoch:1 Step:161/238 Loss:0.3345009684562683\n",
      "Epoch:1 Step:181/238 Loss:0.15711615979671478\n",
      "Epoch:1 Step:201/238 Loss:0.052012138068675995\n",
      "Epoch:1 Step:221/238 Loss:0.03874891251325607\n",
      "Epoch:2 Step:1/238 Loss:0.034609273076057434\n",
      "Epoch:2 Step:21/238 Loss:0.01295490562915802\n",
      "Epoch:2 Step:41/238 Loss:0.039412662386894226\n",
      "Epoch:2 Step:61/238 Loss:0.018687324598431587\n",
      "Epoch:2 Step:81/238 Loss:0.011424683034420013\n",
      "Epoch:2 Step:101/238 Loss:0.006650739815086126\n",
      "Epoch:2 Step:121/238 Loss:0.013876188546419144\n",
      "Epoch:2 Step:141/238 Loss:0.007715730462223291\n",
      "Epoch:2 Step:161/238 Loss:1.0193264484405518\n",
      "Epoch:2 Step:181/238 Loss:0.015324069187045097\n",
      "Epoch:2 Step:201/238 Loss:0.08586928993463516\n",
      "Epoch:2 Step:221/238 Loss:0.008185351267457008\n",
      "Training finished cleanly.\n",
      "[0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "79.3168880455408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9236    0.7535    0.8300       353\n",
      "    Positive     0.6360    0.8736    0.7361       174\n",
      "\n",
      "    accuracy                         0.7932       527\n",
      "   macro avg     0.7798    0.8136    0.7830       527\n",
      "weighted avg     0.8286    0.7932    0.7990       527\n",
      "\n",
      "Epoch:0 Step:1/238 Loss:0.7442431449890137\n",
      "Epoch:0 Step:21/238 Loss:0.3110446333885193\n",
      "Epoch:0 Step:41/238 Loss:0.2180045247077942\n",
      "Epoch:0 Step:61/238 Loss:0.11863602697849274\n",
      "Epoch:0 Step:81/238 Loss:0.11776690185070038\n",
      "Epoch:0 Step:101/238 Loss:0.04143873229622841\n",
      "Epoch:0 Step:121/238 Loss:0.36265242099761963\n",
      "Epoch:0 Step:141/238 Loss:0.028131969273090363\n",
      "Epoch:0 Step:161/238 Loss:0.04471464082598686\n",
      "Epoch:0 Step:181/238 Loss:0.029162555932998657\n",
      "Epoch:0 Step:201/238 Loss:0.042724184691905975\n",
      "Epoch:0 Step:221/238 Loss:0.02435944601893425\n",
      "Epoch:1 Step:1/238 Loss:0.07051049917936325\n",
      "Epoch:1 Step:21/238 Loss:0.6458213329315186\n",
      "Epoch:1 Step:41/238 Loss:0.014382787980139256\n",
      "Epoch:1 Step:61/238 Loss:0.028499089181423187\n",
      "Epoch:1 Step:81/238 Loss:0.02768527902662754\n",
      "Epoch:1 Step:101/238 Loss:0.005745031405240297\n",
      "Epoch:1 Step:121/238 Loss:0.34776240587234497\n",
      "Epoch:1 Step:141/238 Loss:0.029984647408127785\n",
      "Epoch:1 Step:161/238 Loss:0.009449277073144913\n",
      "Epoch:1 Step:181/238 Loss:0.007756918668746948\n",
      "Epoch:1 Step:201/238 Loss:0.012706396169960499\n",
      "Epoch:1 Step:221/238 Loss:0.0918298214673996\n",
      "Epoch:2 Step:1/238 Loss:0.20915082097053528\n",
      "Epoch:2 Step:21/238 Loss:0.009002810344099998\n",
      "Epoch:2 Step:41/238 Loss:0.2218053787946701\n",
      "Epoch:2 Step:61/238 Loss:0.0013244322035461664\n",
      "Epoch:2 Step:81/238 Loss:0.1654232144355774\n",
      "Epoch:2 Step:101/238 Loss:0.029303349554538727\n",
      "Epoch:2 Step:121/238 Loss:0.008717842400074005\n",
      "Epoch:2 Step:141/238 Loss:0.0019593064207583666\n",
      "Epoch:2 Step:161/238 Loss:0.003813250456005335\n",
      "Epoch:2 Step:181/238 Loss:0.0014944940339773893\n",
      "Epoch:2 Step:201/238 Loss:0.016074208542704582\n",
      "Epoch:2 Step:221/238 Loss:0.0006337341037578881\n",
      "Training finished cleanly.\n",
      "[0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1]\n",
      "89.18406072106262\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9229    0.9150    0.9189       353\n",
      "    Positive     0.8305    0.8448    0.8376       174\n",
      "\n",
      "    accuracy                         0.8918       527\n",
      "   macro avg     0.8767    0.8799    0.8783       527\n",
      "weighted avg     0.8924    0.8918    0.8921       527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/238 Loss:0.5223184823989868\n",
      "Epoch:0 Step:21/238 Loss:0.09298431873321533\n",
      "Epoch:0 Step:41/238 Loss:0.035009901970624924\n",
      "Epoch:0 Step:61/238 Loss:0.03562426567077637\n",
      "Epoch:0 Step:81/238 Loss:0.42511019110679626\n",
      "Epoch:0 Step:101/238 Loss:0.0465020090341568\n",
      "Epoch:0 Step:121/238 Loss:0.49784302711486816\n",
      "Epoch:0 Step:141/238 Loss:0.012174705974757671\n",
      "Epoch:0 Step:161/238 Loss:0.010272837243974209\n",
      "Epoch:0 Step:181/238 Loss:0.4228875935077667\n",
      "Epoch:0 Step:201/238 Loss:0.49552032351493835\n",
      "Epoch:0 Step:221/238 Loss:0.03526245057582855\n",
      "Epoch:1 Step:1/238 Loss:0.4553293287754059\n",
      "Epoch:1 Step:21/238 Loss:0.043228186666965485\n",
      "Epoch:1 Step:41/238 Loss:0.5003255605697632\n",
      "Epoch:1 Step:61/238 Loss:0.4311849772930145\n",
      "Epoch:1 Step:81/238 Loss:0.014116243459284306\n",
      "Epoch:1 Step:101/238 Loss:0.05658484250307083\n",
      "Epoch:1 Step:121/238 Loss:0.024988247081637383\n",
      "Epoch:1 Step:141/238 Loss:0.03547721728682518\n",
      "Epoch:1 Step:161/238 Loss:0.009592103771865368\n",
      "Epoch:1 Step:181/238 Loss:0.16934093832969666\n",
      "Epoch:1 Step:201/238 Loss:0.050198785960674286\n",
      "Epoch:1 Step:221/238 Loss:0.028489716351032257\n",
      "Epoch:2 Step:1/238 Loss:0.10561378300189972\n",
      "Epoch:2 Step:21/238 Loss:0.3439284861087799\n",
      "Epoch:2 Step:41/238 Loss:0.0037865384947508574\n",
      "Epoch:2 Step:61/238 Loss:0.004668118432164192\n",
      "Epoch:2 Step:81/238 Loss:0.04200040549039841\n",
      "Epoch:2 Step:101/238 Loss:0.012117430567741394\n",
      "Epoch:2 Step:121/238 Loss:0.5193812251091003\n",
      "Epoch:2 Step:141/238 Loss:0.350024938583374\n",
      "Epoch:2 Step:161/238 Loss:0.024758076295256615\n",
      "Epoch:2 Step:181/238 Loss:0.011972776614129543\n",
      "Epoch:2 Step:201/238 Loss:0.003682316280901432\n",
      "Epoch:2 Step:221/238 Loss:0.003977629356086254\n",
      "Training finished cleanly.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "97.15370018975332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9715    1.0000    0.9856       512\n",
      "    Positive     0.0000    0.0000    0.0000        15\n",
      "\n",
      "    accuracy                         0.9715       527\n",
      "   macro avg     0.4858    0.5000    0.4928       527\n",
      "weighted avg     0.9439    0.9715    0.9575       527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/238 Loss:0.5478124618530273\n",
      "Epoch:0 Step:21/238 Loss:0.08670975267887115\n",
      "Epoch:0 Step:41/238 Loss:0.06301571428775787\n",
      "Epoch:0 Step:61/238 Loss:0.04263300448656082\n",
      "Epoch:0 Step:81/238 Loss:0.513891339302063\n",
      "Epoch:0 Step:101/238 Loss:0.02086064964532852\n",
      "Epoch:0 Step:121/238 Loss:0.04829645901918411\n",
      "Epoch:0 Step:141/238 Loss:0.029289541766047478\n",
      "Epoch:0 Step:161/238 Loss:0.054860636591911316\n",
      "Epoch:0 Step:181/238 Loss:0.04901476949453354\n",
      "Epoch:0 Step:201/238 Loss:0.028043227270245552\n",
      "Epoch:0 Step:221/238 Loss:0.6917788982391357\n",
      "Epoch:1 Step:1/238 Loss:0.022634487599134445\n",
      "Epoch:1 Step:21/238 Loss:0.0033662302885204554\n",
      "Epoch:1 Step:41/238 Loss:0.046737924218177795\n",
      "Epoch:1 Step:61/238 Loss:0.00989456009119749\n",
      "Epoch:1 Step:81/238 Loss:0.006972828414291143\n",
      "Epoch:1 Step:101/238 Loss:0.056075289845466614\n",
      "Epoch:1 Step:121/238 Loss:0.005457428749650717\n",
      "Epoch:1 Step:141/238 Loss:0.045359399169683456\n",
      "Epoch:1 Step:161/238 Loss:0.06723704189062119\n",
      "Epoch:1 Step:181/238 Loss:0.043206751346588135\n",
      "Epoch:1 Step:201/238 Loss:0.009093057364225388\n",
      "Epoch:1 Step:221/238 Loss:0.0021910727955400944\n",
      "Epoch:2 Step:1/238 Loss:0.042291309684515\n",
      "Epoch:2 Step:21/238 Loss:0.04371367022395134\n",
      "Epoch:2 Step:41/238 Loss:0.06987842172384262\n",
      "Epoch:2 Step:61/238 Loss:0.0008387390989810228\n",
      "Epoch:2 Step:81/238 Loss:0.0270465649664402\n",
      "Epoch:2 Step:101/238 Loss:0.0024333694018423557\n",
      "Epoch:2 Step:121/238 Loss:0.01356205902993679\n",
      "Epoch:2 Step:141/238 Loss:0.002698113676160574\n",
      "Epoch:2 Step:161/238 Loss:0.0006545319920405746\n",
      "Epoch:2 Step:181/238 Loss:0.0010896376334130764\n",
      "Epoch:2 Step:201/238 Loss:0.0005859601078554988\n",
      "Epoch:2 Step:221/238 Loss:0.014355436898767948\n",
      "Training finished cleanly.\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "97.15370018975332\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9715    1.0000    0.9856       512\n",
      "    Positive     0.0000    0.0000    0.0000        15\n",
      "\n",
      "    accuracy                         0.9715       527\n",
      "   macro avg     0.4858    0.5000    0.4928       527\n",
      "weighted avg     0.9439    0.9715    0.9575       527\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/238 Loss:0.9107115864753723\n",
      "Epoch:0 Step:21/238 Loss:0.7162814140319824\n",
      "Epoch:0 Step:41/238 Loss:0.1513647735118866\n",
      "Epoch:0 Step:61/238 Loss:0.4388476610183716\n",
      "Epoch:0 Step:81/238 Loss:0.360135555267334\n",
      "Epoch:0 Step:101/238 Loss:0.07532905787229538\n",
      "Epoch:0 Step:121/238 Loss:0.02814149111509323\n",
      "Epoch:0 Step:141/238 Loss:0.20087702572345734\n",
      "Epoch:0 Step:161/238 Loss:0.4415345788002014\n",
      "Epoch:0 Step:181/238 Loss:0.06737054139375687\n",
      "Epoch:0 Step:201/238 Loss:0.09842225164175034\n",
      "Epoch:0 Step:221/238 Loss:0.23344916105270386\n",
      "Epoch:1 Step:1/238 Loss:0.14144939184188843\n",
      "Epoch:1 Step:21/238 Loss:0.042725369334220886\n",
      "Epoch:1 Step:41/238 Loss:0.1011171042919159\n",
      "Epoch:1 Step:61/238 Loss:0.027470458298921585\n",
      "Epoch:1 Step:81/238 Loss:0.030651791021227837\n",
      "Epoch:1 Step:101/238 Loss:0.18801680207252502\n",
      "Epoch:1 Step:121/238 Loss:0.029772497713565826\n",
      "Epoch:1 Step:141/238 Loss:0.03129936754703522\n",
      "Epoch:1 Step:161/238 Loss:0.011643197387456894\n",
      "Epoch:1 Step:181/238 Loss:0.031049925833940506\n",
      "Epoch:1 Step:201/238 Loss:0.06702390313148499\n",
      "Epoch:1 Step:221/238 Loss:0.008015896193683147\n",
      "Epoch:2 Step:1/238 Loss:0.016178656369447708\n",
      "Epoch:2 Step:21/238 Loss:0.024036016315221786\n",
      "Epoch:2 Step:41/238 Loss:0.008610818535089493\n",
      "Epoch:2 Step:61/238 Loss:0.006007646210491657\n",
      "Epoch:2 Step:81/238 Loss:0.15942125022411346\n",
      "Epoch:2 Step:101/238 Loss:0.021235527470707893\n",
      "Epoch:2 Step:121/238 Loss:0.004539779387414455\n",
      "Epoch:2 Step:141/238 Loss:0.005484136287122965\n",
      "Epoch:2 Step:161/238 Loss:0.642361044883728\n",
      "Epoch:2 Step:181/238 Loss:0.006282822694629431\n",
      "Epoch:2 Step:201/238 Loss:0.05175589770078659\n",
      "Epoch:2 Step:221/238 Loss:0.019047139212489128\n",
      "Training finished cleanly.\n",
      "[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "95.06641366223909\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9348    0.9837    0.9586       306\n",
      "    Positive     0.9756    0.9050    0.9390       221\n",
      "\n",
      "    accuracy                         0.9507       527\n",
      "   macro avg     0.9552    0.9443    0.9488       527\n",
      "weighted avg     0.9519    0.9507    0.9504       527\n",
      "\n",
      "Epoch:0 Step:1/238 Loss:0.8134238123893738\n",
      "Epoch:0 Step:21/238 Loss:0.33743521571159363\n",
      "Epoch:0 Step:41/238 Loss:0.3756581246852875\n",
      "Epoch:0 Step:61/238 Loss:0.30118632316589355\n",
      "Epoch:0 Step:81/238 Loss:0.4011955261230469\n",
      "Epoch:0 Step:101/238 Loss:0.0955195277929306\n",
      "Epoch:0 Step:121/238 Loss:0.12485852092504501\n",
      "Epoch:0 Step:141/238 Loss:0.10261903703212738\n",
      "Epoch:0 Step:161/238 Loss:0.10655926167964935\n",
      "Epoch:0 Step:181/238 Loss:0.06047641485929489\n",
      "Epoch:0 Step:201/238 Loss:0.026235975325107574\n",
      "Epoch:0 Step:221/238 Loss:0.05766554921865463\n",
      "Epoch:1 Step:1/238 Loss:0.043372467160224915\n",
      "Epoch:1 Step:21/238 Loss:0.02198134735226631\n",
      "Epoch:1 Step:41/238 Loss:0.0043383995071053505\n",
      "Epoch:1 Step:61/238 Loss:0.05880950018763542\n",
      "Epoch:1 Step:81/238 Loss:0.4439123272895813\n",
      "Epoch:1 Step:101/238 Loss:0.0328168123960495\n",
      "Epoch:1 Step:121/238 Loss:0.040630146861076355\n",
      "Epoch:1 Step:141/238 Loss:0.1307138055562973\n",
      "Epoch:1 Step:161/238 Loss:0.5911858081817627\n",
      "Epoch:1 Step:181/238 Loss:0.040383562445640564\n",
      "Epoch:1 Step:201/238 Loss:0.14193488657474518\n",
      "Epoch:1 Step:221/238 Loss:0.07121653854846954\n",
      "Epoch:2 Step:1/238 Loss:0.01934586465358734\n",
      "Epoch:2 Step:21/238 Loss:0.019637199118733406\n",
      "Epoch:2 Step:41/238 Loss:0.030472515150904655\n",
      "Epoch:2 Step:61/238 Loss:0.5444573163986206\n",
      "Epoch:2 Step:81/238 Loss:0.013067137449979782\n",
      "Epoch:2 Step:101/238 Loss:0.007433002814650536\n",
      "Epoch:2 Step:121/238 Loss:0.003206044901162386\n",
      "Epoch:2 Step:141/238 Loss:0.00393618643283844\n",
      "Epoch:2 Step:161/238 Loss:0.011024540290236473\n",
      "Epoch:2 Step:181/238 Loss:0.022433657199144363\n",
      "Epoch:2 Step:201/238 Loss:0.09798052906990051\n",
      "Epoch:2 Step:221/238 Loss:0.0018746961141005158\n",
      "Training finished cleanly.\n",
      "[0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "93.54838709677419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative     0.9096    0.9869    0.9467       306\n",
      "    Positive     0.9795    0.8643    0.9183       221\n",
      "\n",
      "    accuracy                         0.9355       527\n",
      "   macro avg     0.9446    0.9256    0.9325       527\n",
      "weighted avg     0.9389    0.9355    0.9348       527\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BertForSequenceClassification' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     55\u001b[39m     time.sleep(\u001b[32m5\u001b[39m)\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name,model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./saved_models/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1964\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1962\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1963\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1964\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1965\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1966\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'BertForSequenceClassification' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def prepare_data(task_id):    \n",
    "    folder_train_path = train_path + '/' + task_id\n",
    "\n",
    "    all_train_files = []\n",
    "    for root, dirs, files in os.walk(folder_train_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_train_files.append(full_path)\n",
    "\n",
    "    dfs = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    folder_test_path = test_path + '/' + task_id\n",
    "\n",
    "    all_test_files = []\n",
    "    for root, dirs, files in os.walk(folder_test_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_test_files.append(full_path)\n",
    "\n",
    "    dfs_train = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    dfs_test = [pd.read_csv(file) for file in all_test_files]\n",
    "\n",
    "    df_train = pd.concat(dfs_train, ignore_index=True)\n",
    "\n",
    "    df_test  = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "    return df_train,df_test\n",
    "\n",
    "\n",
    "models = {}\n",
    "for task_id in task_list:\n",
    "\n",
    "    df_train ,df_test = prepare_data(task_id)\n",
    "\n",
    "    model_class = AutoModelForSequenceClassification\n",
    "    model = run_training(model_name,model_class,tokenizer,df_train,df_test)\n",
    "\n",
    "    evaluate_model(model=model,model_class=model_class,df_train=df_test,tokenizer=tokenizer)\n",
    "    key = f'text_only_{task_id}'\n",
    "    models[key] = model\n",
    "    time.sleep(5)\n",
    "\n",
    "    model_class = BertClassifierWithFeatures    \n",
    "\n",
    "    model = run_training(model_name,model_class,tokenizer,df_train,df_test)\n",
    "    evaluate_model(model=model,model_class=model_class,df_train=df_test,tokenizer=tokenizer)\n",
    "    key = f'text_and_features_{task_id}'\n",
    "    models[key] = model\n",
    "    time.sleep(5)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9ef4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,model in models.items():\n",
    "    path = f\"./saved_models/{name}\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "    if hasattr(model, \"save_pretrained\"):  # Hugging Face model\n",
    "        model.save_pretrained(path)\n",
    "    else:  # Custom torch.nn model\n",
    "        torch.save(model.state_dict(), os.path.join(path, \"model.pt\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
