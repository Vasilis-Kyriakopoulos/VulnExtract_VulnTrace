{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60f681c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/VulnExtractData/ffmpeg_test/Train',\n",
       " './data/VulnExtractData/ffmpeg_test/Test')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = './data/VulnExtractData/ffmpeg_test'\n",
    "train_path = data_path + '/Train'\n",
    "test_path = data_path + '/Test'\n",
    "train_path,test_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "785322e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_list = ['AF','BF','CL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98f7fbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "\n",
    "\n",
    "def train_model_svm(X_train,Y_train,X_test,Y_test):\n",
    "    param_grid = {\n",
    "        'svc__C': [0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'svc__degree': [2, 3, 4],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    }\n",
    "    \n",
    "\n",
    "    pipeline = make_pipeline(TfidfVectorizer(ngram_range=(1,1)), SVC(probability=True))\n",
    "    grid_search = GridSearchCV(pipeline,param_grid,verbose = 2,n_jobs = -1)\n",
    "\n",
    "    grid_search.fit(X_train,Y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    Y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, digits=4))\n",
    "\n",
    "def train_model_svm_with_features(X_train,Y_train,X_test,Y_test):\n",
    "    param_grid = {\n",
    "        'svc__C': [0.1, 1, 10, 100],\n",
    "        'svc__kernel': ['linear', 'rbf', 'poly'],\n",
    "        'svc__degree': [2, 3, 4],\n",
    "        'svc__gamma': ['scale', 'auto']\n",
    "    }\n",
    "\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('tfidf', TfidfVectorizer(), 'Description'),  # only your text column name here\n",
    "        ('scaler', StandardScaler(), X_train.columns[1:])\n",
    "    ], remainder='drop')\n",
    "\n",
    "    pipeline = make_pipeline(preprocessor, SVC(probability=True))\n",
    "\n",
    "    grid_search = GridSearchCV(pipeline,param_grid,verbose = 2,n_jobs = -1)\n",
    "\n",
    "    grid_search.fit(X_train,Y_train)\n",
    "\n",
    "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "\n",
    "    Y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "    print(classification_report(Y_test, Y_pred, digits=4))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fe482e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_model_svm_with_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#train_model_svm(X_train,Y_train,X_test,Y_test)\u001b[39;00m\n\u001b[32m     55\u001b[39m X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mtrain_model_svm_with_features\u001b[49m(X_train,Y_train,X_test,Y_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'train_model_svm_with_features' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def prepare_data(task_id,with_features = False):    \n",
    "    folder_train_path = train_path + '/' + task_id\n",
    "\n",
    "    all_train_files = []\n",
    "    for root, dirs, files in os.walk(folder_train_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_train_files.append(full_path)\n",
    "\n",
    "    dfs = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    folder_test_path = test_path + '/' + task_id\n",
    "\n",
    "    all_test_files = []\n",
    "    for root, dirs, files in os.walk(folder_test_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_test_files.append(full_path)\n",
    "\n",
    "    dfs_train = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    dfs_test = [pd.read_csv(file) for file in all_test_files]\n",
    "\n",
    "    df_train = pd.concat(dfs_train, ignore_index=True)\n",
    "\n",
    "    df_test  = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "    #use only Description for training\n",
    "    X_train_descriptions = df_train['Description']\n",
    "    X_test_descriptions = df_test['Description']\n",
    "\n",
    "    if with_features == True:\n",
    "        X_train_features = df_train.drop(columns=['Description', 'label', 'CVE_ID'])\n",
    "        X_test_features = df_test.drop(columns=['Description', 'label', 'CVE_ID'])\n",
    "\n",
    "        X_train = pd.concat([X_train_descriptions,X_train_features],axis=1)\n",
    "        X_test = pd.concat([X_test_descriptions,X_test_features],axis=1)\n",
    "    else:\n",
    "        X_train = X_train_descriptions\n",
    "        X_test = X_test_descriptions\n",
    "\n",
    "    Y_train = df_train['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "    Y_test = df_test['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "   \n",
    "    return X_train,Y_train,X_test,Y_test\n",
    "\n",
    "\n",
    "\n",
    "for task_id in task_list:\n",
    "    X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=False)\n",
    "    #train_model_svm(X_train,Y_train,X_test,Y_test)\n",
    "    X_train,Y_train,X_test,Y_test = prepare_data(task_id, with_features=True)\n",
    "    train_model_svm_with_features(X_train,Y_train,X_test,Y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a32fd1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "2d27f199",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.texts = df['Description']\n",
    "        self.labels =  df['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "\n",
    "class TextFeatureDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        feature_cols = [col for col in df.columns if col not in ['Description', 'CVE_ID', 'label']]\n",
    "        self.max_len = max_len\n",
    "        self.features = df[feature_cols].astype(float)\n",
    "        self.texts = df['Description']\n",
    "        self.labels =  df['label'].apply(lambda x:0 if x==4 else 1).values\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.texts[index]\n",
    "        labels = self.labels[index]\n",
    "        features = self.features.iloc[index].to_numpy(dtype=np.float32, copy=True)\n",
    "        features = torch.from_numpy(features).contiguous() \n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'attention_mask': encoding['attention_mask'].squeeze(0),\n",
    "            'features': torch.tensor(features, dtype=torch.float),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }    \n",
    "\n",
    "class BertClassifierWithFeatures(nn.Module):\n",
    "    def __init__(self,bert_model_name='bert-base-uncased',num_additional_features=16, num_classes=2):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name,num_labels=num_classes)\n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size + num_additional_features, num_classes)\n",
    "        \n",
    "    def forward(self,input_ids,attention_mask,features):\n",
    "        outputs = self.bert(input_ids,attention_mask = attention_mask)\n",
    "        cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        combined = torch.cat((cls_embedding, features), dim=1)\n",
    "        outputs = self.classifier(combined)\n",
    "        return outputs\n",
    "\n",
    "def collate_fn(batch):\n",
    "    input_ids = [item['input_ids'] for item in batch]\n",
    "    attention_mask = [item['attention_mask'] for item in batch]\n",
    "    labels = [item['labels'] for item in batch]\n",
    "    features = [item['features'] for item in batch]\n",
    "    \n",
    "  \n",
    "    input_ids = pad_sequence(input_ids, batch_first=True, padding_value=0)\n",
    "    attention_mask = pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
    "    labels = torch.stack(labels)\n",
    "    features = torch.stack(features)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'features': features,\n",
    "        'labels': labels\n",
    "    }\n",
    "\n",
    "def train_model(model,model_class ,train_loader, device, optimizer, scheduler, epochs=1):\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        for i,batch in enumerate(train_loader):\n",
    "\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            features = batch['features'].to(device)\n",
    "        \n",
    "            model.zero_grad()\n",
    "            if model_class  ==  BertClassifierWithFeatures:\n",
    "                outputs = model(input_ids = input_ids,attention_mask = attention_mask,features=features)\n",
    "                loss = nn.CrossEntropyLoss()(outputs, labels)\n",
    "            else:\n",
    "                outputs = model(input_ids,attention_mask = attention_mask,labels=labels)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            if i % 1 == 0 :\n",
    "                print(f'Epoch:{epoch} Step:{i+1}/{len(train_loader)} Loss:{loss.item()}')\n",
    "            \n",
    "\n",
    "        \n",
    "def run_training(model_name,model_class,tokenizer,df_train,df_val,epochs = 1,learning_rate = 5e-5):\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()      # releases cached GPU memory back to the driver\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if model_class  == BertClassifierWithFeatures:\n",
    "        train_dataset = TextFeatureDataset(df_train,tokenizer)\n",
    "        val_dataset = TextFeatureDataset(df_val,tokenizer)\n",
    "    else:\n",
    "        train_dataset = TextDataset(df_train,tokenizer)\n",
    "        val_dataset = TextDataset(df_val,tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn,pin_memory=False,num_workers=0)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn,pin_memory=False,num_workers=0)\n",
    "\n",
    "    if model_class == BertClassifierWithFeatures:\n",
    "        model = model_class(model_name,num_additional_features = 16, num_classes=2)\n",
    "        \n",
    "    else: \n",
    "        model = model_class.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    train_model(model,model_class=model_class,train_loader=train_loader, device=device,optimizer=optimizer, scheduler=scheduler, epochs=epochs)\n",
    "    print(\"AS\")\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished cleanly.\")\n",
    "    \n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    predictions, true_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            outputs = model(input_ids,attention_mask = attention_mask)\n",
    "            preds = torch.argmax(outputs.logits,dim = 1)\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "            print(n_correct)\n",
    "    print(predictions,true_labels)\n",
    "    print(100* n_correct/ n_samples)\n",
    "    print(classification_report(true_labels, predictions, target_names=['Negative', 'Positive'], digits=4))\n",
    "    return 100* n_correct/ n_samples\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "2d540084",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5558/1513292851.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'features': torch.tensor(features, dtype=torch.float),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/238 Loss:0.5780289173126221\n",
      "Epoch:0 Step:2/238 Loss:1.574992060661316\n",
      "Epoch:0 Step:3/238 Loss:0.35559147596359253\n",
      "Epoch:0 Step:4/238 Loss:0.29194343090057373\n",
      "Epoch:0 Step:5/238 Loss:0.17179684340953827\n",
      "Epoch:0 Step:6/238 Loss:0.5586874485015869\n",
      "Epoch:0 Step:7/238 Loss:0.3265194296836853\n",
      "Epoch:0 Step:8/238 Loss:0.21222633123397827\n",
      "Epoch:0 Step:9/238 Loss:0.329462468624115\n",
      "Epoch:0 Step:10/238 Loss:1.0307537317276\n",
      "Epoch:0 Step:11/238 Loss:0.9044013023376465\n",
      "Epoch:0 Step:12/238 Loss:0.8807015419006348\n",
      "Epoch:0 Step:13/238 Loss:0.6211198568344116\n",
      "Epoch:0 Step:14/238 Loss:0.6834836006164551\n",
      "Epoch:0 Step:15/238 Loss:0.31621885299682617\n",
      "Epoch:0 Step:16/238 Loss:0.2966001629829407\n",
      "Epoch:0 Step:17/238 Loss:0.4231061637401581\n",
      "Epoch:0 Step:18/238 Loss:0.5673868656158447\n",
      "Epoch:0 Step:19/238 Loss:0.6263540983200073\n",
      "Epoch:0 Step:20/238 Loss:1.0530940294265747\n",
      "Epoch:0 Step:21/238 Loss:1.3573473691940308\n",
      "Epoch:0 Step:22/238 Loss:0.8558032512664795\n",
      "Epoch:0 Step:23/238 Loss:0.12958502769470215\n",
      "Epoch:0 Step:24/238 Loss:0.3578651249408722\n",
      "Epoch:0 Step:25/238 Loss:0.4343630373477936\n",
      "Epoch:0 Step:26/238 Loss:0.6995442509651184\n",
      "Epoch:0 Step:27/238 Loss:0.5124136209487915\n",
      "Epoch:0 Step:28/238 Loss:0.34550076723098755\n",
      "Epoch:0 Step:29/238 Loss:0.6455517411231995\n",
      "Epoch:0 Step:30/238 Loss:0.5144020318984985\n",
      "Epoch:0 Step:31/238 Loss:0.37666311860084534\n",
      "Epoch:0 Step:32/238 Loss:0.7111914157867432\n",
      "Epoch:0 Step:33/238 Loss:0.4760150611400604\n",
      "Epoch:0 Step:34/238 Loss:0.537381649017334\n",
      "Epoch:0 Step:35/238 Loss:0.3035723865032196\n",
      "Epoch:0 Step:36/238 Loss:0.039846546947956085\n",
      "Epoch:0 Step:37/238 Loss:0.04452143609523773\n",
      "Epoch:0 Step:38/238 Loss:0.4324009418487549\n",
      "Epoch:0 Step:39/238 Loss:1.0744163990020752\n",
      "Epoch:0 Step:40/238 Loss:0.2770925760269165\n",
      "Epoch:0 Step:41/238 Loss:0.7201931476593018\n",
      "Epoch:0 Step:42/238 Loss:0.3688991665840149\n",
      "Epoch:0 Step:43/238 Loss:0.6744041442871094\n",
      "Epoch:0 Step:44/238 Loss:0.5772063732147217\n",
      "Epoch:0 Step:45/238 Loss:0.5647849440574646\n",
      "Epoch:0 Step:46/238 Loss:0.3939998745918274\n",
      "Epoch:0 Step:47/238 Loss:0.5385726690292358\n",
      "Epoch:0 Step:48/238 Loss:0.15967881679534912\n",
      "Epoch:0 Step:49/238 Loss:0.013296898454427719\n",
      "Epoch:0 Step:50/238 Loss:1.1315585374832153\n",
      "Epoch:0 Step:51/238 Loss:0.20671874284744263\n",
      "Epoch:0 Step:52/238 Loss:0.44025561213493347\n",
      "Epoch:0 Step:53/238 Loss:0.3663072884082794\n",
      "Epoch:0 Step:54/238 Loss:0.39801856875419617\n",
      "Epoch:0 Step:55/238 Loss:0.0922086089849472\n",
      "Epoch:0 Step:56/238 Loss:0.428646981716156\n",
      "Epoch:0 Step:57/238 Loss:1.4446239471435547\n",
      "Epoch:0 Step:58/238 Loss:0.102225162088871\n",
      "Epoch:0 Step:59/238 Loss:0.23542913794517517\n",
      "Epoch:0 Step:60/238 Loss:0.027189964428544044\n",
      "Epoch:0 Step:61/238 Loss:0.31394580006599426\n",
      "Epoch:0 Step:62/238 Loss:0.43931952118873596\n",
      "Epoch:0 Step:63/238 Loss:0.5573546886444092\n",
      "Epoch:0 Step:64/238 Loss:0.33820074796676636\n",
      "Epoch:0 Step:65/238 Loss:1.3190927505493164\n",
      "Epoch:0 Step:66/238 Loss:0.10307836532592773\n",
      "Epoch:0 Step:67/238 Loss:0.3047195076942444\n",
      "Epoch:0 Step:68/238 Loss:0.2050195038318634\n",
      "Epoch:0 Step:69/238 Loss:1.7637007236480713\n",
      "Epoch:0 Step:70/238 Loss:0.6423139572143555\n",
      "Epoch:0 Step:71/238 Loss:0.6304920315742493\n",
      "Epoch:0 Step:72/238 Loss:0.15207064151763916\n",
      "Epoch:0 Step:73/238 Loss:0.3196723461151123\n",
      "Epoch:0 Step:74/238 Loss:0.3475150167942047\n",
      "Epoch:0 Step:75/238 Loss:0.30321308970451355\n",
      "Epoch:0 Step:76/238 Loss:0.23675847053527832\n",
      "Epoch:0 Step:77/238 Loss:0.188906729221344\n",
      "Epoch:0 Step:78/238 Loss:0.34379151463508606\n",
      "Epoch:0 Step:79/238 Loss:0.3191927671432495\n",
      "Epoch:0 Step:80/238 Loss:0.5069996118545532\n",
      "Epoch:0 Step:81/238 Loss:0.9134454727172852\n",
      "Epoch:0 Step:82/238 Loss:0.7995386719703674\n",
      "Epoch:0 Step:83/238 Loss:0.5631130933761597\n",
      "Epoch:0 Step:84/238 Loss:0.27267980575561523\n",
      "Epoch:0 Step:85/238 Loss:0.2769027352333069\n",
      "Epoch:0 Step:86/238 Loss:0.32454022765159607\n",
      "Epoch:0 Step:87/238 Loss:0.9049420356750488\n",
      "Epoch:0 Step:88/238 Loss:0.6064603328704834\n",
      "Epoch:0 Step:89/238 Loss:0.2961798906326294\n",
      "Epoch:0 Step:90/238 Loss:0.07164430618286133\n",
      "Epoch:0 Step:91/238 Loss:0.4033670127391815\n",
      "Epoch:0 Step:92/238 Loss:0.32604074478149414\n",
      "Epoch:0 Step:93/238 Loss:0.3298991322517395\n",
      "Epoch:0 Step:94/238 Loss:0.35058319568634033\n",
      "Epoch:0 Step:95/238 Loss:0.3136199116706848\n",
      "Epoch:0 Step:96/238 Loss:0.4988453686237335\n",
      "Epoch:0 Step:97/238 Loss:1.0465048551559448\n",
      "Epoch:0 Step:98/238 Loss:0.320419579744339\n",
      "Epoch:0 Step:99/238 Loss:1.0615369081497192\n",
      "Epoch:0 Step:100/238 Loss:0.2983207404613495\n",
      "Epoch:0 Step:101/238 Loss:0.458837628364563\n",
      "Epoch:0 Step:102/238 Loss:0.16666248440742493\n",
      "Epoch:0 Step:103/238 Loss:0.39136555790901184\n",
      "Epoch:0 Step:104/238 Loss:0.42544931173324585\n",
      "Epoch:0 Step:105/238 Loss:0.49409520626068115\n",
      "Epoch:0 Step:106/238 Loss:0.22394514083862305\n",
      "Epoch:0 Step:107/238 Loss:0.7188456654548645\n",
      "Epoch:0 Step:108/238 Loss:0.3675602078437805\n",
      "Epoch:0 Step:109/238 Loss:0.5088386535644531\n",
      "Epoch:0 Step:110/238 Loss:0.35692352056503296\n",
      "Epoch:0 Step:111/238 Loss:0.37516364455223083\n",
      "Epoch:0 Step:112/238 Loss:0.4471849501132965\n",
      "Epoch:0 Step:113/238 Loss:0.3795892596244812\n",
      "Epoch:0 Step:114/238 Loss:0.2998112142086029\n",
      "Epoch:0 Step:115/238 Loss:0.03466220200061798\n",
      "Epoch:0 Step:116/238 Loss:0.07937721908092499\n",
      "Epoch:0 Step:117/238 Loss:0.3521144390106201\n",
      "Epoch:0 Step:118/238 Loss:0.4295538067817688\n",
      "Epoch:0 Step:119/238 Loss:0.04927893355488777\n",
      "Epoch:0 Step:120/238 Loss:0.011376271024346352\n",
      "Epoch:0 Step:121/238 Loss:0.5700019001960754\n",
      "Epoch:0 Step:122/238 Loss:0.12638887763023376\n",
      "Epoch:0 Step:123/238 Loss:0.6971108913421631\n",
      "Epoch:0 Step:124/238 Loss:1.0059280395507812\n",
      "Epoch:0 Step:125/238 Loss:0.1767055094242096\n",
      "Epoch:0 Step:126/238 Loss:0.8386271595954895\n",
      "Epoch:0 Step:127/238 Loss:0.6422597169876099\n",
      "Epoch:0 Step:128/238 Loss:0.4037051200866699\n",
      "Epoch:0 Step:129/238 Loss:0.08872213959693909\n",
      "Epoch:0 Step:130/238 Loss:0.38463708758354187\n",
      "Epoch:0 Step:131/238 Loss:0.09722216427326202\n",
      "Epoch:0 Step:132/238 Loss:0.1166621744632721\n",
      "Epoch:0 Step:133/238 Loss:0.17025384306907654\n",
      "Epoch:0 Step:134/238 Loss:0.09701637178659439\n",
      "Epoch:0 Step:135/238 Loss:0.21499128639698029\n",
      "Epoch:0 Step:136/238 Loss:0.17444762587547302\n",
      "Epoch:0 Step:137/238 Loss:0.38921797275543213\n",
      "Epoch:0 Step:138/238 Loss:0.07862857729196548\n",
      "Epoch:0 Step:139/238 Loss:0.1328071802854538\n",
      "Epoch:0 Step:140/238 Loss:0.2188928723335266\n",
      "Epoch:0 Step:141/238 Loss:0.21515682339668274\n",
      "Epoch:0 Step:142/238 Loss:0.0390188992023468\n",
      "Epoch:0 Step:143/238 Loss:0.10916061699390411\n",
      "Epoch:0 Step:144/238 Loss:0.13724155724048615\n",
      "Epoch:0 Step:145/238 Loss:0.41080957651138306\n",
      "Epoch:0 Step:146/238 Loss:0.3903810381889343\n",
      "Epoch:0 Step:147/238 Loss:0.25753653049468994\n",
      "Epoch:0 Step:148/238 Loss:0.059304188936948776\n",
      "Epoch:0 Step:149/238 Loss:0.09597146511077881\n",
      "Epoch:0 Step:150/238 Loss:0.0899612158536911\n",
      "Epoch:0 Step:151/238 Loss:0.5358533263206482\n",
      "Epoch:0 Step:152/238 Loss:0.4515858590602875\n",
      "Epoch:0 Step:153/238 Loss:0.7449145317077637\n",
      "Epoch:0 Step:154/238 Loss:0.09212814271450043\n",
      "Epoch:0 Step:155/238 Loss:0.7196925282478333\n",
      "Epoch:0 Step:156/238 Loss:0.48507821559906006\n",
      "Epoch:0 Step:157/238 Loss:0.38545072078704834\n",
      "Epoch:0 Step:158/238 Loss:0.4586401581764221\n",
      "Epoch:0 Step:159/238 Loss:0.28848934173583984\n",
      "Epoch:0 Step:160/238 Loss:0.8451398611068726\n",
      "Epoch:0 Step:161/238 Loss:0.3327518403530121\n",
      "Epoch:0 Step:162/238 Loss:0.28009673953056335\n",
      "Epoch:0 Step:163/238 Loss:1.1639869213104248\n",
      "Epoch:0 Step:164/238 Loss:0.5967303514480591\n",
      "Epoch:0 Step:165/238 Loss:0.10938753187656403\n",
      "Epoch:0 Step:166/238 Loss:0.0438908189535141\n",
      "Epoch:0 Step:167/238 Loss:0.34566953778266907\n",
      "Epoch:0 Step:168/238 Loss:2.5216827392578125\n",
      "Epoch:0 Step:169/238 Loss:1.7971278429031372\n",
      "Epoch:0 Step:170/238 Loss:0.5542570352554321\n",
      "Epoch:0 Step:171/238 Loss:0.21155749261379242\n",
      "Epoch:0 Step:172/238 Loss:0.10496276617050171\n",
      "Epoch:0 Step:173/238 Loss:0.05701640993356705\n",
      "Epoch:0 Step:174/238 Loss:0.03627752512693405\n",
      "Epoch:0 Step:175/238 Loss:0.07149291783571243\n",
      "Epoch:0 Step:176/238 Loss:0.48308447003364563\n",
      "Epoch:0 Step:177/238 Loss:1.201732873916626\n",
      "Epoch:0 Step:178/238 Loss:0.14706383645534515\n",
      "Epoch:0 Step:179/238 Loss:0.821568489074707\n",
      "Epoch:0 Step:180/238 Loss:0.8600590229034424\n",
      "Epoch:0 Step:181/238 Loss:0.4138668179512024\n",
      "Epoch:0 Step:182/238 Loss:1.0896053314208984\n",
      "Epoch:0 Step:183/238 Loss:0.36438530683517456\n",
      "Epoch:0 Step:184/238 Loss:0.3470984101295471\n",
      "Epoch:0 Step:185/238 Loss:0.18716320395469666\n",
      "Epoch:0 Step:186/238 Loss:0.598029375076294\n",
      "Epoch:0 Step:187/238 Loss:0.2200504094362259\n",
      "Epoch:0 Step:188/238 Loss:0.13718658685684204\n",
      "Epoch:0 Step:189/238 Loss:0.266973614692688\n",
      "Epoch:0 Step:190/238 Loss:0.059025224298238754\n",
      "Epoch:0 Step:191/238 Loss:0.40527594089508057\n",
      "Epoch:0 Step:192/238 Loss:0.09165821969509125\n",
      "Epoch:0 Step:193/238 Loss:0.17813828587532043\n",
      "Epoch:0 Step:194/238 Loss:0.4676021933555603\n",
      "Epoch:0 Step:195/238 Loss:0.3480914235115051\n",
      "Epoch:0 Step:196/238 Loss:0.6498987674713135\n",
      "Epoch:0 Step:197/238 Loss:0.16778074204921722\n",
      "Epoch:0 Step:198/238 Loss:0.5771154165267944\n",
      "Epoch:0 Step:199/238 Loss:0.4412825107574463\n",
      "Epoch:0 Step:200/238 Loss:0.31086456775665283\n",
      "Epoch:0 Step:201/238 Loss:0.37284278869628906\n",
      "Epoch:0 Step:202/238 Loss:1.2381582260131836\n",
      "Epoch:0 Step:203/238 Loss:0.9586111307144165\n",
      "Epoch:0 Step:204/238 Loss:0.29138052463531494\n",
      "Epoch:0 Step:205/238 Loss:0.21479228138923645\n",
      "Epoch:0 Step:206/238 Loss:0.3741508424282074\n",
      "Epoch:0 Step:207/238 Loss:0.2447279840707779\n",
      "Epoch:0 Step:208/238 Loss:0.24877893924713135\n",
      "Epoch:0 Step:209/238 Loss:0.7886893153190613\n",
      "Epoch:0 Step:210/238 Loss:0.23385831713676453\n",
      "Epoch:0 Step:211/238 Loss:0.2089889794588089\n",
      "Epoch:0 Step:212/238 Loss:0.19981124997138977\n",
      "Epoch:0 Step:213/238 Loss:0.15543752908706665\n",
      "Epoch:0 Step:214/238 Loss:0.1479281187057495\n",
      "Epoch:0 Step:215/238 Loss:0.15313491225242615\n",
      "Epoch:0 Step:216/238 Loss:0.10913213342428207\n",
      "Epoch:0 Step:217/238 Loss:0.11002427339553833\n",
      "Epoch:0 Step:218/238 Loss:0.10773023962974548\n",
      "Epoch:0 Step:219/238 Loss:0.49870580434799194\n",
      "Epoch:0 Step:220/238 Loss:0.7233680486679077\n",
      "Epoch:0 Step:221/238 Loss:0.1629241555929184\n",
      "Epoch:0 Step:222/238 Loss:0.17226076126098633\n",
      "Epoch:0 Step:223/238 Loss:0.13259929418563843\n",
      "Epoch:0 Step:224/238 Loss:0.03728538751602173\n",
      "Epoch:0 Step:225/238 Loss:0.03933893144130707\n",
      "Epoch:0 Step:226/238 Loss:0.03639722615480423\n",
      "Epoch:0 Step:227/238 Loss:0.03235902264714241\n",
      "Epoch:0 Step:228/238 Loss:0.03776343911886215\n",
      "Epoch:0 Step:229/238 Loss:0.49523061513900757\n",
      "Epoch:0 Step:230/238 Loss:0.023274630308151245\n",
      "Epoch:0 Step:231/238 Loss:0.03686576336622238\n",
      "Epoch:0 Step:232/238 Loss:0.027089223265647888\n",
      "Epoch:0 Step:233/238 Loss:0.09090879559516907\n",
      "Epoch:0 Step:234/238 Loss:0.5852710008621216\n",
      "Epoch:0 Step:235/238 Loss:0.36422017216682434\n",
      "Epoch:0 Step:236/238 Loss:0.29510045051574707\n",
      "Epoch:0 Step:237/238 Loss:0.6325411200523376\n",
      "Epoch:0 Step:238/238 Loss:0.13604533672332764\n",
      "AS\n",
      "Training finished cleanly.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5558/1513292851.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'features': torch.tensor(features, dtype=torch.float),\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (8x774 and 784x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[263]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     49\u001b[39m model_class = BertClassifierWithFeatures\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#val_dataset = TextFeatureDataset(df_test,tokenizer)\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[262]\u001b[39m\u001b[32m, line 165\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model_name, model_class, tokenizer, df_train, df_val, epochs, learning_rate)\u001b[39m\n\u001b[32m    161\u001b[39m total_steps = \u001b[38;5;28mlen\u001b[39m(train_loader) * epochs\n\u001b[32m    163\u001b[39m scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=\u001b[32m0\u001b[39m, num_training_steps=total_steps)\n\u001b[32m--> \u001b[39m\u001b[32m165\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAS\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    167\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[262]\u001b[39m\u001b[32m, line 124\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, model_class, train_loader, device, optimizer, scheduler, epochs)\u001b[39m\n\u001b[32m    122\u001b[39m model.zero_grad()\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_class  ==  BertClassifierWithFeatures:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    125\u001b[39m     loss = nn.CrossEntropyLoss()(outputs, labels)\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[262]\u001b[39m\u001b[32m, line 87\u001b[39m, in \u001b[36mBertClassifierWithFeatures.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, features)\u001b[39m\n\u001b[32m     85\u001b[39m cls_embedding = outputs.last_hidden_state[:, \u001b[32m0\u001b[39m, :]\n\u001b[32m     86\u001b[39m combined = torch.cat((cls_embedding, features), dim=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/torch/nn/modules/linear.py:134\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m    131\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    132\u001b[39m \u001b[33;03m    Runs the forward pass.\u001b[39;00m\n\u001b[32m    133\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: mat1 and mat2 shapes cannot be multiplied (8x774 and 784x2)"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "model_name = 'bert-base-uncased'\n",
    "model_class = AutoModelForSequenceClassification\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def prepare_data(task_id,with_features = False):    \n",
    "    folder_train_path = train_path + '/' + task_id\n",
    "\n",
    "    all_train_files = []\n",
    "    for root, dirs, files in os.walk(folder_train_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_train_files.append(full_path)\n",
    "\n",
    "    dfs = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    folder_test_path = test_path + '/' + task_id\n",
    "\n",
    "    all_test_files = []\n",
    "    for root, dirs, files in os.walk(folder_test_path):\n",
    "        for file in files:\n",
    "            full_path = os.path.join(root, file)\n",
    "            all_test_files.append(full_path)\n",
    "\n",
    "    dfs_train = [pd.read_csv(file) for file in all_train_files]\n",
    "\n",
    "    dfs_test = [pd.read_csv(file) for file in all_test_files]\n",
    "\n",
    "    df_train = pd.concat(dfs_train, ignore_index=True)\n",
    "\n",
    "    df_test  = pd.concat(dfs_test, ignore_index=True)\n",
    "\n",
    "    return df_train,df_test\n",
    "\n",
    "for task_id in task_list:\n",
    "    df_train ,df_test = prepare_data(task_id, with_features=False)\n",
    "    #model = run_training(model_name,model_class,tokenizer,df_train,df_test)\n",
    "    #val_dataset = TextDataset(df_test,tokenizer)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "    #evaluate_model(model=model,test_loader=val_loader)\n",
    "\n",
    "\n",
    "    df_train ,df_test = prepare_data(task_id, with_features=True)\n",
    "    model_class = BertClassifierWithFeatures\n",
    "    \n",
    "    #val_dataset = TextFeatureDataset(df_test,tokenizer)\n",
    "    #val_loader = DataLoader(val_dataset, batch_size=8, collate_fn=collate_fn)\n",
    "    run_training(model_name,model_class,tokenizer,df_train,df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a56894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5558/2091467243.py:72: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  'features': torch.tensor(features, dtype=torch.float),\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "for batch in train_loader:\n",
    "    i = 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
