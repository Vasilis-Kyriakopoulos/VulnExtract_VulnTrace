{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f27513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\CONDA\\envs\\tf_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec59a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/VulnTraceData/finetune_dataset/'\n",
    "model_name = 'DeepSoftwareAnalytics/CoCoSoDa'\n",
    "\n",
    "project_list = ['binutils', 'ffmpeg', 'libarchive', 'libxml2', 'systemd', 'tcpdump']\n",
    "# model_name_list = ['cocosoda', 'unixcoder-base', 'graphcodebert-base', 'codebert-base', 'roberta-base', 'roberta-code']\n",
    "model_name_list = ['cocosoda']\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def read_data(data_path,project_name):    \n",
    "    df_train = pd.read_csv(data_path+project_name+'_train.csv')\n",
    "    df_val = pd.read_csv(data_path+project_name+'_validation.csv')\n",
    "    return df_train,df_val\n",
    "\n",
    "df_train,df_val = read_data(data_path=data_path,project_name= 'binutils')\n",
    "\n",
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = df['text']\n",
    "        self.code = df['code']\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.text[index]\n",
    "        code = self.code[index]\n",
    "        \n",
    "        text_encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "\n",
    "        code_encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids_text': text_encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'text_attention_mask': code_encoding['attention_mask'].squeeze(0),\n",
    "            'input_ids_code': code_encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0),\n",
    "        }\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_input_ids = [item['input_ids_text'] for item in batch]\n",
    "    text_attention_mask = [item['text_attention_mask'] for item in batch]\n",
    "    code_input_ids = [item['input_ids_code'] for item in batch]\n",
    "    code_attention_mask = [item['code_attention_mask'] for item in batch]\n",
    "    \n",
    "    text_input_ids = pad_sequence(text_input_ids, batch_first=True, padding_value=0)\n",
    "    text_attention_mask = pad_sequence(text_attention_mask, batch_first=True, padding_value=0)\n",
    "    code_input_ids = pad_sequence(code_input_ids, batch_first=True, padding_value=0)\n",
    "    code_attention_mask = pad_sequence(code_attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'code_input_ids': code_input_ids,\n",
    "        'code_attention_mask': code_attention_mask\n",
    "    }    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3254f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss_curves(train_accuracies,train_losses,val_accuracies,val_losses):\n",
    "  \"\"\"\n",
    "  Returns separate loss curves for training and validation metrics.\n",
    "\n",
    "  Args:\n",
    "    history: TensorFlow model History object (see: https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/History)\n",
    "  \"\"\" \n",
    "  loss = train_losses\n",
    "  val_loss = val_losses\n",
    "\n",
    "  accuracy = train_accuracies\n",
    "  val_accuracy = val_accuracies\n",
    "\n",
    "  epochs = range(len(train_accuracies))\n",
    "\n",
    "  # Plot loss\n",
    "  plt.plot(epochs, loss, label='training_loss')\n",
    "  plt.plot(epochs, val_loss, label='val_loss')\n",
    "  plt.title('Loss')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "\n",
    "  # Plot accuracy\n",
    "  plt.figure()\n",
    "  plt.plot(epochs, accuracy, label='training_accuracy')\n",
    "  plt.plot(epochs, val_accuracy, label='val_accuracy')\n",
    "  plt.title('Accuracy')\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.legend()\n",
    "  plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3661087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/289 Loss:0.33128416538238525\n",
      "Epoch:0 Step:21/289 Loss:0.08385612815618515\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 158\u001b[39m\n\u001b[32m    156\u001b[39m gc.collect()\n\u001b[32m    157\u001b[39m torch.cuda.empty_cache()\n\u001b[32m--> \u001b[39m\u001b[32m158\u001b[39m model = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model_name, tokenizer, df_train, df_val, epochs, learning_rate)\u001b[39m\n\u001b[32m     87\u001b[39m model = AutoModel.from_pretrained(model_name)\n\u001b[32m     89\u001b[39m optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay =\u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m train_losses,train_accuracies,val_losses,val_accuracies= \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m plot_loss_curves(train_accuracies,train_losses,val_accuracies,val_losses)\n\u001b[32m    101\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, df_train, df_val, device, optimizer, tokenizer, epochs)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#loss = outputs.loss\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#preds = torch.argmax(outputs.logits,dim =1 )\u001b[39;00m\n\u001b[32m     54\u001b[39m total_loss += loss.item()\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m optimizer.step()\n\u001b[32m     57\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CONDA\\envs\\tf_env\\Lib\\site-packages\\torch\\_tensor.py:625\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    616\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    617\u001b[39m         Tensor.backward,\n\u001b[32m    618\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    623\u001b[39m         inputs=inputs,\n\u001b[32m    624\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m625\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    626\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CONDA\\envs\\tf_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\CONDA\\envs\\tf_env\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    840\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m841\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    844\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    845\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "train_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "val_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "test_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "def train_model(model,df_train,df_val, device, optimizer,tokenizer,epochs=3):\n",
    "    model.to(device)\n",
    "    train_losses=[]\n",
    "    train_accuracies= []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    train_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "    val_dataset = TextCodeDataset(df_val,tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for i,batch in enumerate(train_loader):\n",
    "            \n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            \n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs,loss,preds = None,None,None\n",
    "            \n",
    "            text_outputs = model(text_input_ids,attention_mask = text_attention_mask)\n",
    "            code_outputs = model(code_input_ids,attention_mask = code_attention_mask)\n",
    "\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            code_embeddings = code_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "            code_embeddings = F.normalize(code_embeddings, dim=-1)\n",
    "\n",
    "            scores = torch.einsum('md,nd->mn', text_embeddings, code_embeddings) * 10\n",
    "            labels = torch.arange(scores.shape[0]).to(scores.device)\n",
    "            loss = nn.CrossEntropyLoss()(scores, labels)\n",
    "   \n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            n_samples += len(text_input_ids)\n",
    "            n_correct += len(text_input_ids)\n",
    "\n",
    "            if i % 20 == 0 :\n",
    "                print(f'Epoch:{epoch} Step:{i+1}/{len(train_loader)} Loss:{loss.item()}')\n",
    "\n",
    "        results = evaluate_model(model=model,df_test=df_val,tokenizer=tokenizer)\n",
    "\n",
    "        avg_loss = total_loss/len(train_loader)\n",
    "        accuracy = n_correct/n_samples\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        print(train_losses)\n",
    "        val_accuracies.append(results['accuracy'])\n",
    "        val_losses.append(results['loss'])\n",
    "\n",
    "    print(train_losses,train_accuracies,val_accuracies,val_losses)\n",
    "    return train_losses,train_accuracies,val_losses,val_accuracies\n",
    "\n",
    "\n",
    "        \n",
    "def run_training(model_name,tokenizer,df_train,df_val,epochs = 3,learning_rate = 2e-5):\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()      # releases cached GPU memory back to the driver\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay =0.01)\n",
    "    \n",
    "    train_losses,train_accuracies,val_losses,val_accuracies= train_model(model,\n",
    "                                          df_train=df_train,\n",
    "                                          df_val = df_val, \n",
    "                                          device=device,\n",
    "                                          optimizer=optimizer, \n",
    "                                          tokenizer=tokenizer, \n",
    "                                          epochs=epochs\n",
    "                                        )\n",
    "    \n",
    "    plot_loss_curves(train_accuracies,train_losses,val_accuracies,val_losses)\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model,df_test,tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_dataset = TextCodeDataset(df_test,tokenizer)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    total_loss = 0\n",
    "    predictions =  []\n",
    "    true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            \n",
    "            outputs,loss,preds = None,None,None\n",
    "                \n",
    "            text_outputs = model(text_input_ids,attention_mask = text_attention_mask)\n",
    "            code_outputs = model(code_input_ids,attention_mask = code_attention_mask)\n",
    "\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            code_embeddings = code_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "            code_embeddings = F.normalize(code_embeddings, dim=-1)\n",
    "\n",
    "            scores = torch.einsum('md,nd->mn', text_embeddings, code_embeddings) * 10\n",
    "            labels = torch.arange(scores.shape[0]).to(scores.device)\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            loss = nn.CrossEntropyLoss()(scores, labels)\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss/len(test_loader) \n",
    "    acc = n_correct/ n_samples\n",
    "    print(acc)\n",
    "    return {'accuracy':acc,\"loss\":avg_loss}\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = run_training(model_name=model_name,tokenizer=tokenizer,df_train=df_train,df_val=df_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
