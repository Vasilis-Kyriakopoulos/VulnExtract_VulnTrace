{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f27513",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vasilis/anaconda3/envs/tfgpu/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-11-08 21:48:47.823491: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import os\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import TensorDataset, Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import AdamW\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec59a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = './data/VulnTraceData/finetune_dataset/'\n",
    "model_name = 'DeepSoftwareAnalytics/CoCoSoDa'\n",
    "\n",
    "project_list = ['binutils', 'ffmpeg', 'libarchive', 'libxml2', 'systemd', 'tcpdump']\n",
    "# model_name_list = ['cocosoda', 'unixcoder-base', 'graphcodebert-base', 'codebert-base', 'roberta-base', 'roberta-code']\n",
    "model_name_list = ['cocosoda']\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def read_data(data_path,project_name):    \n",
    "    df_train = pd.read_csv(data_path+project_name+'_train.csv')\n",
    "    df_val = pd.read_csv(data_path+project_name+'_validation.csv')\n",
    "    return df_train,df_val\n",
    "\n",
    "df_train,df_val = read_data(data_path=data_path,project_name= 'binutils')\n",
    "\n",
    "class TextCodeDataset(Dataset):\n",
    "    def __init__(self,df,tokenizer,max_len = 512):\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.text = df['text']\n",
    "        self.code = df['code']\n",
    "\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        text = self.text[index]\n",
    "        code = self.code[index]\n",
    "        \n",
    "        text_encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "\n",
    "        code_encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True,\n",
    "            padding=False \n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids_text': text_encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'text_attention_mask': code_encoding['attention_mask'].squeeze(0),\n",
    "            'input_ids_code': code_encoding['input_ids'].squeeze(0),  # Remove the batch dimension\n",
    "            'code_attention_mask': code_encoding['attention_mask'].squeeze(0),\n",
    "        }\n",
    "    \n",
    "\n",
    "def collate_fn(batch):\n",
    "    text_input_ids = [item['input_ids_text'] for item in batch]\n",
    "    text_attention_mask = [item['text_attention_mask'] for item in batch]\n",
    "    code_input_ids = [item['input_ids_code'] for item in batch]\n",
    "    code_attention_mask = [item['code_attention_mask'] for item in batch]\n",
    "    \n",
    "    text_input_ids = pad_sequence(text_input_ids, batch_first=True, padding_value=0)\n",
    "    text_attention_mask = pad_sequence(text_attention_mask, batch_first=True, padding_value=0)\n",
    "    code_input_ids = pad_sequence(code_input_ids, batch_first=True, padding_value=0)\n",
    "    code_attention_mask = pad_sequence(code_attention_mask, batch_first=True, padding_value=0)\n",
    "\n",
    "    return {\n",
    "        'text_input_ids': text_input_ids,\n",
    "        'text_attention_mask': text_attention_mask,\n",
    "        'code_input_ids': code_input_ids,\n",
    "        'code_attention_mask': code_attention_mask\n",
    "    }    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3661087a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Step:1/289 Loss:0.40190958976745605\n",
      "Epoch:0 Step:21/289 Loss:0.20171162486076355\n",
      "Epoch:0 Step:41/289 Loss:0.012511484324932098\n",
      "Epoch:0 Step:61/289 Loss:0.18212535977363586\n",
      "Epoch:0 Step:81/289 Loss:0.01042841374874115\n",
      "Epoch:0 Step:101/289 Loss:0.012651723809540272\n",
      "Epoch:0 Step:121/289 Loss:0.006943820510059595\n",
      "Epoch:0 Step:141/289 Loss:0.008318429812788963\n",
      "Epoch:0 Step:161/289 Loss:0.0051935454830527306\n",
      "Epoch:0 Step:181/289 Loss:0.016839109361171722\n",
      "Epoch:0 Step:201/289 Loss:0.006000175140798092\n",
      "Epoch:0 Step:221/289 Loss:0.016403838992118835\n",
      "Epoch:0 Step:241/289 Loss:0.011416382156312466\n",
      "Epoch:0 Step:261/289 Loss:0.18725073337554932\n",
      "Epoch:0 Step:281/289 Loss:0.003771288087591529\n",
      "stoxastis\n",
      "0.9258620689655173\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of classes, 0, does not match size of target_names, 2. Try specifying the labels parameter",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 159\u001b[39m\n\u001b[32m    157\u001b[39m gc.collect()\n\u001b[32m    158\u001b[39m torch.cuda.empty_cache()\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m model = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 91\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model_name, tokenizer, df_train, df_val, epochs, learning_rate)\u001b[39m\n\u001b[32m     87\u001b[39m model = AutoModel.from_pretrained(model_name)\n\u001b[32m     89\u001b[39m optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay =\u001b[32m0.01\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m train_losses,train_accuracies,val_losses,val_accuracies= \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m                                    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m#plot_loss_curves(train_accuracies,train_losses,val_accuracies,val_losses)\u001b[39;00m\n\u001b[32m    101\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 65\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, df_train, df_val, device, optimizer, tokenizer, epochs)\u001b[39m\n\u001b[32m     62\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i % \u001b[32m20\u001b[39m == \u001b[32m0\u001b[39m :\n\u001b[32m     63\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mEpoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Loss:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m results = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m avg_loss = total_loss/\u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[32m     68\u001b[39m accuracy = n_correct/n_samples\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 153\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, df_test, tokenizer)\u001b[39m\n\u001b[32m    151\u001b[39m acc = n_correct/ n_samples\n\u001b[32m    152\u001b[39m \u001b[38;5;28mprint\u001b[39m(acc)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[43mclassification_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrue_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_names\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mNegative\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPositive\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m:acc,\u001b[33m\"\u001b[39m\u001b[33mloss\u001b[39m\u001b[33m\"\u001b[39m:avg_loss}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:218\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m    214\u001b[39m         skip_parameter_validation=(\n\u001b[32m    215\u001b[39m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m    216\u001b[39m         )\n\u001b[32m    217\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[32m    224\u001b[39m     msg = re.sub(\n\u001b[32m    225\u001b[39m         \u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[33m\\\u001b[39m\u001b[33mw+ must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    226\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must be\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    227\u001b[39m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[32m    228\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/anaconda3/envs/tfgpu/lib/python3.12/site-packages/sklearn/metrics/_classification.py:2970\u001b[39m, in \u001b[36mclassification_report\u001b[39m\u001b[34m(y_true, y_pred, labels, target_names, sample_weight, digits, output_dict, zero_division)\u001b[39m\n\u001b[32m   2964\u001b[39m         warnings.warn(\n\u001b[32m   2965\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mlabels size, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of target_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m\"\u001b[39m.format(\n\u001b[32m   2966\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names)\n\u001b[32m   2967\u001b[39m             )\n\u001b[32m   2968\u001b[39m         )\n\u001b[32m   2969\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2970\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   2971\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mNumber of classes, \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m, does not match size of \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2972\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mtarget_names, \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m. Try specifying the labels \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   2973\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mparameter\u001b[39m\u001b[33m\"\u001b[39m.format(\u001b[38;5;28mlen\u001b[39m(labels), \u001b[38;5;28mlen\u001b[39m(target_names))\n\u001b[32m   2974\u001b[39m         )\n\u001b[32m   2975\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m target_names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2976\u001b[39m     target_names = [\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m % l \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m labels]\n",
      "\u001b[31mValueError\u001b[39m: Number of classes, 0, does not match size of target_names, 2. Try specifying the labels parameter"
     ]
    }
   ],
   "source": [
    "train_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "val_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "test_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "def train_model(model,df_train,df_val, device, optimizer,tokenizer,epochs=3):\n",
    "    model.to(device)\n",
    "    train_losses=[]\n",
    "    train_accuracies= []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    train_dataset = TextCodeDataset(df_train,tokenizer)\n",
    "    val_dataset = TextCodeDataset(df_val,tokenizer)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn,num_workers=0)\n",
    "    test_loader = DataLoader(train_dataset, batch_size=8, shuffle=False, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    total_steps = len(train_loader) * epochs\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        n_correct = 0\n",
    "        n_samples = 0\n",
    "        total_loss = 0\n",
    "\n",
    "        for i,batch in enumerate(train_loader):\n",
    "            \n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            \n",
    "\n",
    "            model.zero_grad()\n",
    "            outputs,loss,preds = None,None,None\n",
    "            \n",
    "            text_outputs = model(text_input_ids,attention_mask = text_attention_mask)\n",
    "            code_outputs = model(code_input_ids,attention_mask = code_attention_mask)\n",
    "\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            code_embeddings = code_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "            code_embeddings = F.normalize(code_embeddings, dim=-1)\n",
    "\n",
    "            scores = torch.einsum('md,nd->mn', text_embeddings, code_embeddings) * 10\n",
    "            labels = torch.arange(scores.shape[0]).to(scores.device)\n",
    "            loss = nn.CrossEntropyLoss()(scores, labels)\n",
    "   \n",
    "            #loss = outputs.loss\n",
    "            #preds = torch.argmax(outputs.logits,dim =1 )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            n_samples += len(text_input_ids)\n",
    "            n_correct += len(text_input_ids)\n",
    "\n",
    "            if i % 20 == 0 :\n",
    "                print(f'Epoch:{epoch} Step:{i+1}/{len(train_loader)} Loss:{loss.item()}')\n",
    "\n",
    "        results = evaluate_model(model=model,df_test=df_val,tokenizer=tokenizer)\n",
    "\n",
    "        avg_loss = total_loss/len(train_loader)\n",
    "        accuracy = n_correct/n_samples\n",
    "        \n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(accuracy)\n",
    "        print(train_losses)\n",
    "        val_accuracies.append(results['accuracy'])\n",
    "        val_losses.append(results['loss'])\n",
    "\n",
    "    print(train_losses,train_accuracies,val_accuracies,val_losses)\n",
    "    return train_losses,train_accuracies,val_losses,val_accuracies\n",
    "\n",
    "\n",
    "        \n",
    "def run_training(model_name,tokenizer,df_train,df_val,epochs = 3,learning_rate = 2e-5):\n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache()      # releases cached GPU memory back to the driver\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate,weight_decay =0.01)\n",
    "    \n",
    "    train_losses,train_accuracies,val_losses,val_accuracies= train_model(model,\n",
    "                                          df_train=df_train,\n",
    "                                          df_val = df_val, \n",
    "                                          device=device,\n",
    "                                          optimizer=optimizer, \n",
    "                                          tokenizer=tokenizer, \n",
    "                                          epochs=epochs\n",
    "                                        )\n",
    "    \n",
    "    #plot_loss_curves(train_accuracies,train_losses,val_accuracies,val_losses)\n",
    "    torch.cuda.synchronize()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Training finished.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model,df_test,tokenizer):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    val_dataset = TextCodeDataset(df_test,tokenizer)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn,num_workers=0)\n",
    "\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    total_loss = 0\n",
    "    predictions =  []\n",
    "    true_labels = []\n",
    "    print('stoxastis')\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            text_input_ids = batch['text_input_ids'].to(device)\n",
    "            text_attention_mask = batch['text_attention_mask'].to(device)\n",
    "            code_input_ids = batch['code_input_ids'].to(device)\n",
    "            code_attention_mask = batch['code_attention_mask'].to(device)\n",
    "            \n",
    "            outputs,loss,preds = None,None,None\n",
    "                \n",
    "            text_outputs = model(text_input_ids,attention_mask = text_attention_mask)\n",
    "            code_outputs = model(code_input_ids,attention_mask = code_attention_mask)\n",
    "\n",
    "            text_embeddings = text_outputs.last_hidden_state[:, 0, :]\n",
    "            code_embeddings = code_outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "            text_embeddings = F.normalize(text_embeddings, dim=-1)\n",
    "            code_embeddings = F.normalize(code_embeddings, dim=-1)\n",
    "\n",
    "            scores = torch.einsum('md,nd->mn', text_embeddings, code_embeddings) * 10\n",
    "            labels = torch.arange(scores.shape[0]).to(scores.device)\n",
    "            preds = torch.argmax(scores, dim=1)\n",
    "            loss = nn.CrossEntropyLoss()(scores, labels)\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "\n",
    "            n_samples += labels.shape[0]\n",
    "            n_correct += (preds == labels).sum().item()\n",
    "\n",
    "    avg_loss = total_loss/len(test_loader) \n",
    "    acc = n_correct/ n_samples\n",
    "    print(acc)\n",
    "    return {'accuracy':acc,\"loss\":avg_loss}\n",
    "\n",
    "torch.cuda.synchronize()\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "model = run_training(model_name=model_name,tokenizer=tokenizer,df_train=df_train,df_val=df_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
